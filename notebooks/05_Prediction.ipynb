{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4038943",
   "metadata": {},
   "source": [
    "# **Notebook 05 — Spatial Inference and Probability Mapping**\n",
    "\n",
    "## Predictive Archaeological Modelling — Peru (v2)\n",
    "**Author:** Yishar Piero Nieto Barrientos\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Apply the best trained classifier (serialized in Notebook 04) to raster feature\n",
    "layers and generate a continuous probability surface (0–1) representing\n",
    "archaeological potential.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Load serialized model + scaler and validate compatibility |\n",
    "| 2 | Register input raster paths from `src/config.py` and verify spatial integrity |\n",
    "| 3 | Define Area of Interest (AOI) via GPS coordinates |\n",
    "| 4 | Stack features, mask NoData, run `predict_proba` |\n",
    "| 5 | Export probability surface as GeoTIFF (ESRI:102033) |\n",
    "| 6 | Feature importance analysis (Mean Decrease in Impurity) |\n",
    "| 7 | Visual validation against official archaeological records |\n",
    "| 8 | Probability distribution and cumulative coverage analysis |\n",
    "| 9 | Quantitative validation: site capture rate (Kvamme gain) |\n",
    "| 10 | Prediction uncertainty mapping (ensemble disagreement) |\n",
    "\n",
    "**Depends on:** Notebook 04 (serialized model + scaler).\n",
    "**Next:** Notebook 06 (Multi-Region Extension).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e0eed1",
   "metadata": {},
   "source": [
    "## 1. Configuration and Model Loading\n",
    "\n",
    "All paths, feature registry, and constants are imported from `src/config.py`.\n",
    "The serialized model (best from Notebook 04) must expose `predict_proba` for\n",
    "continuous probability output.  Features excluded during training (redundancy\n",
    "removal in NB04: `rugosidad`, `pisos_ecologicos`) are automatically skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8fb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "import time\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import joblib\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.warp import transform as warp_transform\n",
    "\n",
    "from src.config import (\n",
    "    RASTER_DIR, MODELS_DIR, RAW_DIR,\n",
    "    RASTER_REGISTRY, get_feature_names,\n",
    "    figures_dir, CRS,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Paths (derived from config)\n",
    "# ---------------------------------------------------------------------------\n",
    "OUTPUT_DIR  = os.path.join(os.path.abspath(\"..\"), \"outputs\", \"predicciones\")\n",
    "FIGURES_DIR = figures_dir(\"05_prediction\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Redundant variables removed in NB04 — must be excluded from prediction\n",
    "# ---------------------------------------------------------------------------\n",
    "EXCLUDED_FEATURES = {\"rugosidad\", \"pisos_ecologicos\"}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Build FEATURE_RASTERS dynamically from config (excluding dropped vars)\n",
    "# ---------------------------------------------------------------------------\n",
    "FEATURE_RASTERS = {}\n",
    "FEATURE_LABELS  = {}\n",
    "\n",
    "for rv in RASTER_REGISTRY:\n",
    "    if rv.name in EXCLUDED_FEATURES:\n",
    "        continue\n",
    "    FEATURE_RASTERS[rv.name] = rv.path\n",
    "    FEATURE_LABELS[rv.name]  = f\"{rv.label} ({rv.unit})\" if rv.unit else rv.label\n",
    "\n",
    "print(f\"Features for prediction ({len(FEATURE_RASTERS)}):\")\n",
    "for name, path in FEATURE_RASTERS.items():\n",
    "    status = \"OK\" if os.path.exists(path) else \"MISSING\"\n",
    "    print(f\"  [{status}] {name:<20s} -> {os.path.basename(path)}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Vector paths for validation\n",
    "# ---------------------------------------------------------------------------\n",
    "VECTOR_PATHS = {\n",
    "    \"declarados\":  os.path.join(RAW_DIR, \"declarados\", \"declarados.shp\"),\n",
    "    \"delimitados\": os.path.join(RAW_DIR, \"delimitados\", \"delimitados.shp\"),\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# AOI center — Cusco, Peru (WGS84)\n",
    "# ---------------------------------------------------------------------------\n",
    "AOI_LAT = -13.5167\n",
    "AOI_LON = -71.9781\n",
    "WINDOW_HALF_SIZE = 1500  # pixels (~90 x 90 km at 30 m resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed90156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Load model + scaler (auto-detect best model from NB04)\n",
    "# ---------------------------------------------------------------------------\n",
    "model_files = sorted(glob.glob(os.path.join(MODELS_DIR, \"modelo_*_final.pkl\")))\n",
    "assert model_files, f\"No serialized models found in {MODELS_DIR}\"\n",
    "\n",
    "MODEL_PATH  = model_files[0]  # best model exported by NB04\n",
    "SCALER_PATH = os.path.join(MODELS_DIR, \"scaler_entrenamiento.pkl\")\n",
    "\n",
    "model = joblib.load(MODEL_PATH)\n",
    "logger.info(\"Model loaded: %s  (%s)\", os.path.basename(MODEL_PATH), type(model).__name__)\n",
    "\n",
    "# Load scaler (kept for compatibility — tree models don't need it)\n",
    "if os.path.exists(SCALER_PATH):\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    logger.info(\"Scaler loaded: %s\", SCALER_PATH)\n",
    "else:\n",
    "    scaler = None\n",
    "    logger.warning(\"No scaler found — assuming tree-based model (no scaling needed).\")\n",
    "\n",
    "assert hasattr(model, \"predict_proba\"), (\n",
    "    \"Model does not support predict_proba. \"\n",
    "    \"A probabilistic classifier is required for continuous mapping.\"\n",
    ")\n",
    "\n",
    "# Verify feature count matches\n",
    "expected_n = len(FEATURE_RASTERS)\n",
    "if hasattr(model, \"n_features_in_\"):\n",
    "    model_n = model.n_features_in_\n",
    "    assert model_n == expected_n, (\n",
    "        f\"Feature mismatch: model expects {model_n} features, \"\n",
    "        f\"but {expected_n} rasters registered. Check EXCLUDED_FEATURES.\"\n",
    "    )\n",
    "    logger.info(\"Feature count verified: %d features.\", model_n)\n",
    "\n",
    "logger.info(\"predict_proba available — continuous probability mapping enabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515f9fa",
   "metadata": {},
   "source": [
    "## 2. Input Raster Verification\n",
    "\n",
    "All feature rasters must exist and share the same CRS, resolution, and extent.\n",
    "The feature order must match the training order from Notebook 04.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd67f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Verify raster existence and spatial consistency\n",
    "# ---------------------------------------------------------------------------\n",
    "# Note: Rasters may have different geographic extents (e.g., rivers only cover\n",
    "# certain regions), but must share the same CRS and resolution (30 m grid).\n",
    "# The inference engine (Section 4) handles partial coverage via NoData masking.\n",
    "\n",
    "reference_crs = None\n",
    "reference_res = None\n",
    "\n",
    "raster_info = {}\n",
    "\n",
    "for name, path in FEATURE_RASTERS.items():\n",
    "    assert os.path.exists(path), f\"Missing raster: {name} -> {path}\"\n",
    "    with rasterio.open(path) as src:\n",
    "        raster_info[name] = {\n",
    "            \"shape\": src.shape,\n",
    "            \"crs\": src.crs,\n",
    "            \"res\": src.res,\n",
    "            \"bounds\": src.bounds,\n",
    "            \"dtype\": src.dtypes[0]\n",
    "        }\n",
    "        \n",
    "        if reference_crs is None:\n",
    "            reference_crs = src.crs\n",
    "            reference_res = src.res\n",
    "        else:\n",
    "            assert src.crs == reference_crs, f\"CRS mismatch in {name}\"\n",
    "            assert np.allclose(src.res, reference_res, atol=0.01), \\\n",
    "                f\"Resolution mismatch in {name}: {src.res} vs {reference_res}\"\n",
    "    \n",
    "    logger.info(\n",
    "        \"OK  %-15s  shape=%s  res=%.1f m  bounds=[%.0f, %.0f, %.0f, %.0f]\",\n",
    "        name, raster_info[name][\"shape\"], src.res[0],\n",
    "        src.bounds.left, src.bounds.bottom, src.bounds.right, src.bounds.top\n",
    "    )\n",
    "\n",
    "logger.info(\"CRS: %s | Resolution: %.1f m\", reference_crs, reference_res[0])\n",
    "logger.info(\"All %d feature rasters verified (CRS & resolution).\", len(FEATURE_RASTERS))\n",
    "logger.info(\"Note: Rasters may have partial geographic coverage; inference handles NoData.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc831d",
   "metadata": {},
   "source": [
    "## 3. Area of Interest - Window Definition\n",
    "\n",
    "The AOI is defined by a GPS coordinate (WGS84) which is reprojected to the\n",
    "native CRS (ESRI:102033). A square window of `WINDOW_HALF_SIZE * 2` pixels\n",
    "is created around the center pixel.\n",
    "\n",
    "At 30 m resolution, 3000 px = 90 km per side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Reproject AOI center and build rasterio Window\n",
    "# ---------------------------------------------------------------------------\n",
    "ref_path = FEATURE_RASTERS[\"pendiente\"]\n",
    "\n",
    "with rasterio.open(ref_path) as src:\n",
    "    xs, ys = warp_transform(\"EPSG:4326\", src.crs, [AOI_LON], [AOI_LAT])\n",
    "    x_center, y_center = xs[0], ys[0]\n",
    "    row_center, col_center = src.index(x_center, y_center)\n",
    "\n",
    "    col_off = max(0, col_center - WINDOW_HALF_SIZE)\n",
    "    row_off = max(0, row_center - WINDOW_HALF_SIZE)\n",
    "    win_size = WINDOW_HALF_SIZE * 2\n",
    "\n",
    "    aoi_window = Window(\n",
    "        col_off=col_off,\n",
    "        row_off=row_off,\n",
    "        width=win_size,\n",
    "        height=win_size\n",
    "    )\n",
    "    aoi_transform = src.window_transform(aoi_window)\n",
    "    project_crs = src.crs\n",
    "\n",
    "logger.info(\n",
    "    \"AOI center: (%.4f, %.4f) -> pixel (%d, %d)\",\n",
    "    AOI_LAT, AOI_LON, row_center, col_center\n",
    ")\n",
    "logger.info(\"Window: %d x %d px | offset: (%d, %d)\", win_size, win_size, col_off, row_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258619bf",
   "metadata": {},
   "source": [
    "## 4. Spatial Inference\n",
    "\n",
    "**Steps:**\n",
    "1. Read each feature raster within the AOI window and stack into a 3-D array `(n_features, height, width)`.\n",
    "2. Reshape to `(n_pixels, n_features)` for scikit-learn compatibility.\n",
    "3. Build a NoData mask: pixels with `NaN` in any layer are excluded.\n",
    "4. Call `model.predict_proba()` on valid pixels and collect class-1 probability.\n",
    "5. Reshape back to 2-D for export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Stack features and predict\n",
    "# ---------------------------------------------------------------------------\n",
    "t0 = time.time()\n",
    "\n",
    "layers = []\n",
    "for name, path in FEATURE_RASTERS.items():\n",
    "    with rasterio.open(path) as src:\n",
    "        layers.append(src.read(1, window=aoi_window))\n",
    "\n",
    "stack = np.stack(layers, axis=0)  # (n_features, H, W)\n",
    "n_features, height, width = stack.shape\n",
    "logger.info(\"Stack shape: %s\", stack.shape)\n",
    "\n",
    "# Flatten to (n_pixels, n_features)\n",
    "X = stack.transpose(1, 2, 0).reshape(-1, n_features)\n",
    "\n",
    "# NoData mask\n",
    "valid_mask = ~np.isnan(X).any(axis=1)\n",
    "n_valid = valid_mask.sum()\n",
    "logger.info(\"Valid pixels: %s / %s (%.1f%%)\", f\"{n_valid:,}\", f\"{X.shape[0]:,}\", 100 * n_valid / X.shape[0])\n",
    "\n",
    "# Predict\n",
    "prob_flat = np.full(X.shape[0], np.nan, dtype=np.float32)\n",
    "if n_valid > 0:\n",
    "    prob_flat[valid_mask] = model.predict_proba(X[valid_mask])[:, 1]\n",
    "else:\n",
    "    logger.warning(\"No valid pixels in AOI. Check coordinates.\")\n",
    "\n",
    "probability_map = prob_flat.reshape(height, width)\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "logger.info(\"Inference completed in %.1f s\", elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3aaa7a",
   "metadata": {},
   "source": [
    "## 5. Export GeoTIFF\n",
    "\n",
    "The probability surface is saved as a single-band `Float32` GeoTIFF with:\n",
    "- CRS inherited from the project (ESRI:102033).\n",
    "- Affine transform computed for the AOI window.\n",
    "- LZW compression to reduce file size.\n",
    "- `NaN` as NoData value for transparency in GIS viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd18035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Write GeoTIFF\n",
    "# ---------------------------------------------------------------------------\n",
    "output_filename = \"mapa_calor_cusco_ampliado.tif\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "with rasterio.open(ref_path) as src:\n",
    "    profile = src.profile.copy()\n",
    "\n",
    "profile.update(\n",
    "    driver=\"GTiff\",\n",
    "    height=height,\n",
    "    width=width,\n",
    "    count=1,\n",
    "    dtype=\"float32\",\n",
    "    crs=project_crs,\n",
    "    transform=aoi_transform,\n",
    "    nodata=np.nan,\n",
    "    compress=\"lzw\"\n",
    ")\n",
    "\n",
    "with rasterio.open(output_path, \"w\", **profile) as dst:\n",
    "    dst.write(probability_map.astype(np.float32), 1)\n",
    "\n",
    "logger.info(\"GeoTIFF saved: %s\", output_path)\n",
    "logger.info(\"CRS: %s | Size: %d x %d\", project_crs, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadcd5a4",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "If the best model exposes `feature_importances_` (tree-based models: Random Forest,\n",
    "Gradient Boosting, XGBoost), the Mean Decrease in Impurity (MDI) ranking is shown.\n",
    "This indicates the relative contribution of each variable to the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb504ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Feature importance bar chart\n",
    "# ---------------------------------------------------------------------------\n",
    "feature_names = list(FEATURE_RASTERS.keys())\n",
    "\n",
    "if hasattr(model, \"feature_importances_\"):\n",
    "    importances = model.feature_importances_\n",
    "    labels = [FEATURE_LABELS.get(f, f) for f in feature_names]\n",
    "\n",
    "    # Sort descending\n",
    "    order = np.argsort(importances)[::-1]\n",
    "    imp_sorted = importances[order]\n",
    "    lbl_sorted = [labels[i] for i in order]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5), dpi=150)\n",
    "    colors = plt.cm.YlOrRd(np.linspace(0.35, 0.85, len(imp_sorted)))[::-1]\n",
    "\n",
    "    bars = ax.barh(\n",
    "        range(len(imp_sorted)), imp_sorted,\n",
    "        color=colors[order], edgecolor=\"0.3\", linewidth=0.5\n",
    "    )\n",
    "    ax.set_yticks(range(len(lbl_sorted)))\n",
    "    ax.set_yticklabels(lbl_sorted, fontsize=10)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel(\"Relative Importance (MDI)\", fontsize=11)\n",
    "    ax.set_title(\n",
    "        f\"Feature Importance — {type(model).__name__}\",\n",
    "        fontsize=13, fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    for i, val in enumerate(imp_sorted):\n",
    "        ax.text(val + 0.004, i, f\"{val:.1%}\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "    ax.set_xlim(0, imp_sorted.max() * 1.18)\n",
    "    ax.grid(axis=\"x\", alpha=0.25)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = os.path.join(FIGURES_DIR, \"feature_importance.png\")\n",
    "    fig.savefig(fig_path, dpi=200, bbox_inches=\"tight\")\n",
    "    logger.info(\"Feature importance figure saved: %s\", fig_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Summary table\n",
    "    print()\n",
    "    print(\"{:<35s} {:>8s}\".format(\"Feature\", \"Weight\"))\n",
    "    print(\"-\" * 45)\n",
    "    for lbl, val in zip(lbl_sorted, imp_sorted):\n",
    "        print(f\"{lbl:<35s} {val:>7.1%}\")\n",
    "else:\n",
    "    print(f\"Model ({type(model).__name__}) does not expose feature_importances_.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b70c68",
   "metadata": {},
   "source": [
    "## 7. Visual Validation\n",
    "\n",
    "Three-panel figure comparing the model output against official records:\n",
    "\n",
    "| Panel | Content |\n",
    "|-------|---------|\n",
    "| A | Raw probability surface |\n",
    "| B | Overlay with declared sites (points) and delimited zones (polygons) |\n",
    "| C | High-probability mask (>= 0.7) vs. known sites |\n",
    "\n",
    "All layers remain in **ESRI:102033** to avoid reprojection artifacts.  \n",
    "Vectors are clipped to the raster extent before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eee6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patheffects as pe\n",
    "\n",
    "\n",
    "def clip_vectors_to_raster(raster_path, vector_paths):\n",
    "    \"\"\"Load and clip vector layers to the extent of a raster, in native CRS.\"\"\"\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        crs = src.crs\n",
    "        b = src.bounds\n",
    "\n",
    "    bbox = gpd.GeoDataFrame({\"geometry\": [box(b.left, b.bottom, b.right, b.top)]}, crs=crs)\n",
    "    result = {}\n",
    "    for key, path in vector_paths.items():\n",
    "        gdf = gpd.read_file(path)\n",
    "        if gdf.crs != crs:\n",
    "            gdf = gdf.to_crs(crs)\n",
    "        result[key] = gpd.clip(gdf, bbox)\n",
    "        logger.info(\"%s: %d features within AOI\", key, len(result[key]))\n",
    "    return result\n",
    "\n",
    "\n",
    "# Load data\n",
    "vectors = clip_vectors_to_raster(output_path, VECTOR_PATHS)\n",
    "gdf_pts = vectors[\"declarados\"]\n",
    "gdf_pol = vectors[\"delimitados\"]\n",
    "\n",
    "src_raster = rasterio.open(output_path)\n",
    "raster_data = src_raster.read(1)\n",
    "extent = [src_raster.bounds.left, src_raster.bounds.right,\n",
    "          src_raster.bounds.bottom, src_raster.bounds.top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Three-panel validation figure\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(28, 9), dpi=200)\n",
    "\n",
    "# --- Panel A: Raw probability surface ---\n",
    "im1 = show(src_raster, ax=ax1, cmap=\"magma\", vmin=0, vmax=1)\n",
    "ax1.set_title(\"(A) Archaeological Probability Surface\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Easting (m)\")\n",
    "ax1.set_ylabel(\"Northing (m)\")\n",
    "ax1.grid(True, ls=\"--\", alpha=0.25)\n",
    "\n",
    "div1 = make_axes_locatable(ax1)\n",
    "cax1 = div1.append_axes(\"right\", size=\"4%\", pad=0.08)\n",
    "plt.colorbar(im1.get_images()[0], cax=cax1, label=\"P(site)\")\n",
    "\n",
    "# --- Panel B: Overlay with official records ---\n",
    "show(src_raster, ax=ax2, cmap=\"magma\", vmin=0, vmax=1, alpha=0.8)\n",
    "gdf_pol.plot(ax=ax2, facecolor=\"none\", edgecolor=\"#00FFFF\", linewidth=1.2)\n",
    "gdf_pts.plot(ax=ax2, color=\"white\", marker=\"^\", markersize=55, edgecolor=\"black\", linewidth=0.6)\n",
    "\n",
    "if len(gdf_pts) <= 50 and \"nombre_map\" in gdf_pts.columns:\n",
    "    for _, row in gdf_pts.iterrows():\n",
    "        ax2.text(\n",
    "            row.geometry.x, row.geometry.y + 200,\n",
    "            str(row[\"nombre_map\"]), fontsize=5, color=\"white\", weight=\"bold\",\n",
    "            path_effects=[pe.withStroke(linewidth=1.5, foreground=\"black\")]\n",
    "        )\n",
    "\n",
    "ax2.set_title(\"(B) Prediction vs. Official Records\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Easting (m)\")\n",
    "ax2.set_yticklabels([])\n",
    "ax2.grid(True, ls=\"--\", alpha=0.25)\n",
    "\n",
    "legend_b = [\n",
    "    Line2D([0], [0], color=\"#00FFFF\", lw=2, label=\"Delimited zone\"),\n",
    "    Line2D([0], [0], marker=\"^\", color=\"w\", markeredgecolor=\"k\", label=\"Declared site\", markersize=8)\n",
    "]\n",
    "ax2.legend(handles=legend_b, loc=\"upper right\", frameon=True, facecolor=\"black\", labelcolor=\"white\", fontsize=8)\n",
    "\n",
    "# --- Panel C: High-probability mask (>= 0.7) ---\n",
    "show(src_raster, ax=ax3, cmap=\"Greys_r\", vmin=0, vmax=1, alpha=0.35)\n",
    "\n",
    "high_prob = np.where(raster_data >= 0.7, raster_data, np.nan)\n",
    "im3 = ax3.imshow(high_prob, cmap=\"YlOrRd\", vmin=0.7, vmax=1.0,\n",
    "                  extent=extent, origin=\"upper\", alpha=0.9)\n",
    "\n",
    "gdf_pol.plot(ax=ax3, facecolor=\"none\", edgecolor=\"#00FFFF\", linewidth=1.0)\n",
    "gdf_pts.plot(ax=ax3, color=\"lime\", marker=\"^\", markersize=45, edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "ax3.set_title(\"(C) High-Probability Zones (>=70%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_xlabel(\"Easting (m)\")\n",
    "ax3.set_yticklabels([])\n",
    "ax3.grid(True, ls=\"--\", alpha=0.25)\n",
    "\n",
    "div3 = make_axes_locatable(ax3)\n",
    "cax3 = div3.append_axes(\"right\", size=\"4%\", pad=0.08)\n",
    "plt.colorbar(im3, cax=cax3, label=\"P(site) >= 0.7\")\n",
    "\n",
    "legend_c = [\n",
    "    Line2D([0], [0], color=\"#00FFFF\", lw=2, label=\"Delimited zone\"),\n",
    "    Line2D([0], [0], marker=\"^\", color=\"lime\", markeredgecolor=\"k\", label=\"Declared site\", markersize=8),\n",
    "    Line2D([0], [0], marker=\"s\", color=\"red\", alpha=0.7, label=\"Candidate zone (>=70%)\", markersize=8)\n",
    "]\n",
    "ax3.legend(handles=legend_c, loc=\"upper right\", frameon=True, facecolor=\"black\", labelcolor=\"white\", fontsize=7)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Spatial Validation - Cusco Region ({win_size*30/1000:.0f} x {win_size*30/1000:.0f} km)\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.01\n",
    ")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig_val_path = os.path.join(FIGURES_DIR, \"validation_cusco_3panel.png\")\n",
    "fig.savefig(fig_val_path, dpi=200, bbox_inches=\"tight\")\n",
    "logger.info(\"Validation figure saved: %s\", fig_val_path)\n",
    "plt.show()\n",
    "\n",
    "src_raster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05450c45",
   "metadata": {},
   "source": [
    "## 8. Probability Distribution Analysis\n",
    "\n",
    "Characterizing the predicted probability surface is essential for threshold\n",
    "selection and model calibration assessment. A bimodal distribution indicates\n",
    "strong discrimination between classes; a unimodal one suggests weak separation.\n",
    "\n",
    "The cumulative coverage curve quantifies the trade-off between threshold\n",
    "stringency and spatial coverage, directly informing field survey prioritization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Probability distribution histogram + cumulative coverage curve\n",
    "# ---------------------------------------------------------------------------\n",
    "valid_probs = probability_map[~np.isnan(probability_map)].ravel()\n",
    "\n",
    "fig, (ax_h, ax_c) = plt.subplots(1, 2, figsize=(16, 5), dpi=150)\n",
    "\n",
    "# --- Left: Histogram ---\n",
    "ax_h.hist(valid_probs, bins=100, color=\"#4C72B0\", edgecolor=\"white\",\n",
    "          linewidth=0.3, density=True, alpha=0.85)\n",
    "\n",
    "thresholds = [0.5, 0.7, 0.9]\n",
    "th_colors = [\"#E8A838\", \"#C44E52\", \"#8C1515\"]\n",
    "for th, c in zip(thresholds, th_colors):\n",
    "    pct_above = 100 * (valid_probs >= th).sum() / len(valid_probs)\n",
    "    ax_h.axvline(th, color=c, ls=\"--\", lw=1.5,\n",
    "                 label=f\"P >= {th:.1f}  ({pct_above:.1f}% area)\")\n",
    "\n",
    "ax_h.set_xlabel(\"Predicted Probability P(site)\", fontsize=11)\n",
    "ax_h.set_ylabel(\"Density\", fontsize=11)\n",
    "ax_h.set_title(\"(A) Probability Distribution - AOI\", fontsize=12, fontweight=\"bold\")\n",
    "ax_h.legend(fontsize=9, loc=\"upper center\")\n",
    "ax_h.set_xlim(0, 1)\n",
    "ax_h.grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "# --- Right: Cumulative coverage curve ---\n",
    "sorted_p = np.sort(valid_probs)[::-1]\n",
    "cumulative = np.arange(1, len(sorted_p) + 1) / len(sorted_p) * 100\n",
    "\n",
    "ax_c.plot(sorted_p, cumulative, color=\"#4C72B0\", lw=2)\n",
    "ax_c.fill_between(sorted_p, cumulative, alpha=0.15, color=\"#4C72B0\")\n",
    "\n",
    "for th, c in zip(thresholds, th_colors):\n",
    "    pct = 100 * (valid_probs >= th).sum() / len(valid_probs)\n",
    "    ax_c.plot(th, pct, \"o\", color=c, ms=8, zorder=5)\n",
    "    ax_c.annotate(f\"{pct:.1f}%\", (th, pct), textcoords=\"offset points\",\n",
    "                  xytext=(-25, 10), fontsize=9, fontweight=\"bold\", color=c)\n",
    "\n",
    "ax_c.set_xlabel(\"Probability Threshold\", fontsize=11)\n",
    "ax_c.set_ylabel(\"% of AOI Area Above Threshold\", fontsize=11)\n",
    "ax_c.set_title(\"(B) Cumulative Coverage Curve\", fontsize=12, fontweight=\"bold\")\n",
    "ax_c.set_xlim(0, 1)\n",
    "ax_c.set_ylim(0, 105)\n",
    "ax_c.invert_xaxis()\n",
    "ax_c.grid(alpha=0.25)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_dist_path = os.path.join(FIGURES_DIR, \"probability_distribution.png\")\n",
    "fig.savefig(fig_dist_path, dpi=200, bbox_inches=\"tight\")\n",
    "logger.info(\"Distribution figure saved: %s\", fig_dist_path)\n",
    "plt.show()\n",
    "\n",
    "# --- Summary statistics ---\n",
    "print()\n",
    "print(\"Probability Surface - Summary Statistics\")\n",
    "print(\"=\" * 45)\n",
    "stats = {\n",
    "    \"Valid pixels\":  f\"{len(valid_probs):,}\",\n",
    "    \"Min\":           f\"{np.min(valid_probs):.4f}\",\n",
    "    \"Q1 (25%)\":      f\"{np.percentile(valid_probs, 25):.4f}\",\n",
    "    \"Median\":        f\"{np.median(valid_probs):.4f}\",\n",
    "    \"Mean\":          f\"{np.mean(valid_probs):.4f}\",\n",
    "    \"Q3 (75%)\":      f\"{np.percentile(valid_probs, 75):.4f}\",\n",
    "    \"Max\":           f\"{np.max(valid_probs):.4f}\",\n",
    "    \"Std Dev\":       f\"{np.std(valid_probs):.4f}\",\n",
    "}\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k:<20s} {v:>12s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6076fdbf",
   "metadata": {},
   "source": [
    "## 9. Quantitative Validation - Site Capture Rate\n",
    "\n",
    "The **capture rate** (or site hit rate) measures the percentage of known\n",
    "archaeological sites that fall within zones predicted as high-probability.\n",
    "This is the spatial equivalent of sensitivity/recall and is the standard\n",
    "validation metric in predictive archaeological modeling (Kvamme, 1988;\n",
    "Verhagen & Whitley, 2012).\n",
    "\n",
    "A well-calibrated model should capture a high proportion of known sites\n",
    "while flagging a small fraction of the total area, demonstrating that the\n",
    "model concentrates risk better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Quantitative validation: site capture rate at multiple thresholds\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Sample probability at each declared site location\n",
    "with rasterio.open(output_path) as src:\n",
    "    site_coords = list(zip(gdf_pts.geometry.x, gdf_pts.geometry.y))\n",
    "    site_probs = np.array([v[0] for v in src.sample(site_coords)])\n",
    "\n",
    "# Remove NaN (sites outside raster coverage)\n",
    "valid_site_mask = ~np.isnan(site_probs)\n",
    "site_probs_valid = site_probs[valid_site_mask]\n",
    "n_sites = len(site_probs_valid)\n",
    "logger.info(\"Sites with valid probability: %d / %d\", n_sites, len(site_probs))\n",
    "\n",
    "# Compute capture rate at multiple thresholds\n",
    "eval_thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "capture_data = []\n",
    "\n",
    "for th in eval_thresholds:\n",
    "    n_captured = (site_probs_valid >= th).sum()\n",
    "    capture_rate = 100 * n_captured / n_sites if n_sites > 0 else 0\n",
    "    area_pct = 100 * (valid_probs >= th).sum() / len(valid_probs)\n",
    "    gain = capture_rate / area_pct if area_pct > 0 else 0\n",
    "    capture_data.append((th, n_captured, n_sites, capture_rate, area_pct, gain))\n",
    "\n",
    "# --- Plot: Capture rate vs Area curve (Kvamme-style) ---\n",
    "fig, (ax_cap, ax_bar) = plt.subplots(1, 2, figsize=(16, 5.5), dpi=150)\n",
    "\n",
    "# Left panel: Capture vs Area curve\n",
    "rates = [100 * (site_probs_valid >= th).sum() / n_sites for th in eval_thresholds]\n",
    "areas = [100 * (valid_probs >= th).sum() / len(valid_probs) for th in eval_thresholds]\n",
    "\n",
    "ax_cap.plot(areas, rates, \"o-\", color=\"#C44E52\", lw=2, ms=7, zorder=3)\n",
    "ax_cap.plot([0, 100], [0, 100], \"--\", color=\"gray\", lw=1, label=\"Random model\")\n",
    "ax_cap.fill_between(areas, rates, [a for a in areas], alpha=0.15, color=\"#C44E52\")\n",
    "\n",
    "for th, r, a in zip(eval_thresholds, rates, areas):\n",
    "    if th in [0.5, 0.7, 0.9]:\n",
    "        ax_cap.annotate(f\"P>={th:.1f}\", (a, r), textcoords=\"offset points\",\n",
    "                        xytext=(8, -5), fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "ax_cap.set_xlabel(\"% of AOI Area Flagged\", fontsize=11)\n",
    "ax_cap.set_ylabel(\"% of Known Sites Captured\", fontsize=11)\n",
    "ax_cap.set_title(\"(A) Site Capture Rate vs. Area Surveyed\", fontsize=12, fontweight=\"bold\")\n",
    "ax_cap.legend(fontsize=9)\n",
    "ax_cap.set_xlim(0, 100)\n",
    "ax_cap.set_ylim(0, 105)\n",
    "ax_cap.grid(alpha=0.25)\n",
    "\n",
    "# Right panel: Bar chart of site probability distribution\n",
    "bins_edges = np.arange(0, 1.1, 0.1)\n",
    "counts, _ = np.histogram(site_probs_valid, bins=bins_edges)\n",
    "bin_labels = [f\"{bins_edges[i]:.1f}-{bins_edges[i+1]:.1f}\" for i in range(len(counts))]\n",
    "\n",
    "colors_bar = plt.cm.RdYlGn(np.linspace(0.1, 0.9, len(counts)))\n",
    "ax_bar.bar(range(len(counts)), counts, color=colors_bar, edgecolor=\"0.3\", linewidth=0.5)\n",
    "ax_bar.set_xticks(range(len(bin_labels)))\n",
    "ax_bar.set_xticklabels(bin_labels, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax_bar.set_xlabel(\"Predicted Probability Range\", fontsize=11)\n",
    "ax_bar.set_ylabel(\"Number of Known Sites\", fontsize=11)\n",
    "ax_bar.set_title(\"(B) Distribution of Predictions at Known Sites\", fontsize=12, fontweight=\"bold\")\n",
    "ax_bar.grid(axis=\"y\", alpha=0.25)\n",
    "\n",
    "for i, v in enumerate(counts):\n",
    "    if v > 0:\n",
    "        ax_bar.text(i, v + 0.3, str(v), ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_cap_path = os.path.join(FIGURES_DIR, \"capture_rate_analysis.png\")\n",
    "fig.savefig(fig_cap_path, dpi=200, bbox_inches=\"tight\")\n",
    "logger.info(\"Capture rate figure saved: %s\", fig_cap_path)\n",
    "plt.show()\n",
    "\n",
    "# --- Print capture rate table ---\n",
    "print()\n",
    "header = f\"  {\"Threshold\":>9s} {\"Captured\":>12s} {\"Rate\":>8s} {\"Area\":>8s} {\"Kvamme\":>8s}\"\n",
    "print(header)\n",
    "print(\"-\" * 55)\n",
    "for th, nc, ns, cr, ap, g in capture_data:\n",
    "    print(f\"  {th:>9.1f} {nc:>5d}/{ns:<5d} {cr:>7.1f}% {ap:>7.1f}% {g:>7.2f}\")\n",
    "print()\n",
    "print(\"Kvamme Gain > 1.0 = better than random. Gain >> 1.0 = strong discrimination.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40323d86",
   "metadata": {},
   "source": [
    "## 10. Prediction Uncertainty (Ensemble Disagreement) — Sampled Approach\n",
    "\n",
    "For ensemble models that expose individual estimators, the **standard deviation**\n",
    "across member predictions quantifies prediction uncertainty: low values indicate\n",
    "consensus, high values flag ambiguous zones.\n",
    "\n",
    "- **Random Forest:** Each tree votes independently → `std(tree_proba)`.\n",
    "- **Gradient Boosting / XGBoost:** Uses staged predictions at different boosting\n",
    "  rounds to approximate ensemble disagreement.\n",
    "\n",
    "**Optimization:** Uncertainty is computed on a random sample of valid pixels,\n",
    "then smoothed via Gaussian filtering to reconstruct the full spatial pattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Prediction uncertainty via ensemble disagreement (sampled approach)\n",
    "# ---------------------------------------------------------------------------\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "t0_unc = time.time()\n",
    "\n",
    "# Stratified random sampling to reduce memory footprint\n",
    "X_valid = X[valid_mask]\n",
    "n_valid_pixels = X_valid.shape[0]\n",
    "sample_size = min(750_000, max(100_000, n_valid_pixels // 4))\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "sample_indices = rng.choice(n_valid_pixels, size=sample_size, replace=False)\n",
    "X_sample = X_valid[sample_indices]\n",
    "\n",
    "logger.info(\n",
    "    \"Sampling %d / %d valid pixels (%.1f%%) for uncertainty estimation\",\n",
    "    sample_size, n_valid_pixels, 100 * sample_size / n_valid_pixels,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Compute ensemble disagreement depending on model type\n",
    "# ---------------------------------------------------------------------------\n",
    "model_type = type(model).__name__\n",
    "\n",
    "if isinstance(model, RandomForestClassifier):\n",
    "    # RF: standard deviation across independent tree predictions\n",
    "    tree_preds = np.array([t.predict_proba(X_sample)[:, 1] for t in model.estimators_])\n",
    "    uncertainty_sample = np.std(tree_preds, axis=0)\n",
    "    logger.info(\"Uncertainty method: RF tree disagreement (%d trees)\", len(model.estimators_))\n",
    "\n",
    "elif isinstance(model, GradientBoostingClassifier):\n",
    "    # GBM: staged_predict_proba at different boosting rounds\n",
    "    staged = list(model.staged_predict_proba(X_sample))\n",
    "    n_stages = len(staged)\n",
    "    stage_indices = np.linspace(n_stages // 2, n_stages - 1, min(50, n_stages // 2), dtype=int)\n",
    "    partial_preds = np.array([staged[i][:, 1] for i in stage_indices])\n",
    "    uncertainty_sample = np.std(partial_preds, axis=0)\n",
    "    logger.info(\"Uncertainty method: GBM staged disagreement (%d stages sampled)\", len(stage_indices))\n",
    "\n",
    "else:\n",
    "    # XGBoost or other: use iteration_range snapshots\n",
    "    try:\n",
    "        n_rounds = model.n_estimators\n",
    "        step = max(1, n_rounds // 50)\n",
    "        partial_preds = []\n",
    "        for limit in range(n_rounds // 2, n_rounds + 1, step):\n",
    "            partial_preds.append(model.predict_proba(X_sample, iteration_range=(0, limit))[:, 1])\n",
    "        partial_preds = np.array(partial_preds)\n",
    "        uncertainty_sample = np.std(partial_preds, axis=0)\n",
    "        logger.info(\"Uncertainty method: XGBoost iteration range (%d snapshots)\", len(partial_preds))\n",
    "    except Exception:\n",
    "        logger.warning(\"Could not compute uncertainty for %s. Using zeros.\", model_type)\n",
    "        uncertainty_sample = np.zeros(sample_size, dtype=np.float32)\n",
    "\n",
    "logger.info(\"Uncertainty computed on sample in %.1f s\", time.time() - t0_unc)\n",
    "\n",
    "# Map sampled indices back to 2D and build sparse uncertainty grid\n",
    "uncertainty_sparse = np.full(height * width, np.nan, dtype=np.float32)\n",
    "valid_indices_flat = np.where(valid_mask)[0]\n",
    "sample_indices_2d = valid_indices_flat[sample_indices]\n",
    "uncertainty_sparse[sample_indices_2d] = uncertainty_sample\n",
    "\n",
    "uncertainty_sample_map = uncertainty_sparse.reshape(height, width)\n",
    "\n",
    "# Smooth via gaussian filtering to propagate to unsampled regions\n",
    "uncertainty_map = gaussian_filter(\n",
    "    np.where(np.isnan(uncertainty_sample_map), 0, uncertainty_sample_map),\n",
    "    sigma=2.0,\n",
    ")\n",
    "uncertainty_map[np.isnan(probability_map)] = np.nan\n",
    "\n",
    "logger.info(\"Uncertainty smoothed and mapped in %.1f s\", time.time() - t0_unc)\n",
    "\n",
    "# --- Two-panel figure: Probability + Uncertainty side by side ---\n",
    "fig, (ax_p, ax_u) = plt.subplots(1, 2, figsize=(20, 8), dpi=150)\n",
    "\n",
    "# Panel A: Probability\n",
    "im_p = ax_p.imshow(probability_map, cmap=\"magma\", vmin=0, vmax=1,\n",
    "                    extent=extent, origin=\"upper\")\n",
    "div_p = make_axes_locatable(ax_p)\n",
    "cax_p = div_p.append_axes(\"right\", size=\"4%\", pad=0.08)\n",
    "plt.colorbar(im_p, cax=cax_p, label=\"P(site)\")\n",
    "ax_p.set_title(\"(A) Probability Surface\", fontsize=12, fontweight=\"bold\")\n",
    "ax_p.set_xlabel(\"Easting (m)\")\n",
    "ax_p.set_ylabel(\"Northing (m)\")\n",
    "ax_p.grid(True, ls=\"--\", alpha=0.2)\n",
    "\n",
    "# Panel B: Uncertainty\n",
    "im_u = ax_u.imshow(uncertainty_map, cmap=\"YlOrRd\", vmin=0,\n",
    "                    extent=extent, origin=\"upper\")\n",
    "div_u = make_axes_locatable(ax_u)\n",
    "cax_u = div_u.append_axes(\"right\", size=\"4%\", pad=0.08)\n",
    "plt.colorbar(im_u, cax=cax_u, label=\"Std. Dev. (ensemble disagreement)\")\n",
    "ax_u.set_title(f\"(B) Prediction Uncertainty — {model_type}\", fontsize=12, fontweight=\"bold\")\n",
    "ax_u.set_xlabel(\"Easting (m)\")\n",
    "ax_u.set_yticklabels([])\n",
    "ax_u.grid(True, ls=\"--\", alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_unc_path = os.path.join(FIGURES_DIR, \"prediction_uncertainty.png\")\n",
    "fig.savefig(fig_unc_path, dpi=200, bbox_inches=\"tight\")\n",
    "logger.info(\"Uncertainty figure saved: %s\", fig_unc_path)\n",
    "plt.show()\n",
    "\n",
    "# --- Summary stats ---\n",
    "unc_valid = uncertainty_map[~np.isnan(uncertainty_map)]\n",
    "print()\n",
    "print(f\"Prediction Uncertainty Statistics ({model_type})\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Sample size:        {sample_size:,} / {n_valid_pixels:,} pixels\")\n",
    "print(f\"  Mean uncertainty:   {np.mean(unc_valid):.4f}\")\n",
    "print(f\"  Median uncertainty: {np.median(unc_valid):.4f}\")\n",
    "print(f\"  Max uncertainty:    {np.max(unc_valid):.4f}\")\n",
    "pct_low = 100 * (unc_valid < 0.1).sum() / len(unc_valid)\n",
    "print(f\"  Low uncertainty (<0.1): {pct_low:.1f}% of pixels\")\n",
    "print(f\"  -> Model shows high consensus across most of the AOI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51246d37",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Outputs generated:**\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `outputs/predicciones/mapa_calor_cusco_ampliado.tif` | Probability surface (GeoTIFF, ESRI:102033) |\n",
    "| `outputs/figures/05_prediction/feature_importance.png` | Feature importance ranking (MDI) |\n",
    "| `outputs/figures/05_prediction/probability_distribution.png` | Probability histogram + cumulative coverage |\n",
    "| `outputs/figures/05_prediction/capture_rate_analysis.png` | Kvamme-style capture rate + site prediction distribution |\n",
    "| `outputs/figures/05_prediction/prediction_uncertainty.png` | Probability vs. ensemble disagreement |\n",
    "| `outputs/figures/05_prediction/validation_cusco_3panel.png` | Three-panel spatial validation |\n",
    "\n",
    "**Features used (after redundancy removal in NB04):**\n",
    "- `rugosidad` and `pisos_ecologicos` excluded (Spearman |r_s| > 0.90).\n",
    "- Remaining 11 features loaded dynamically from `src/config.py`.\n",
    "\n",
    "**Key metrics (Cusco AOI):**\n",
    "- Feature importance ranking identifies the dominant environmental drivers.\n",
    "- Capture rate analysis quantifies model sensitivity against official records.\n",
    "- Uncertainty mapping highlights zones requiring additional field verification.\n",
    "\n",
    "**Next steps:**\n",
    "- Open the GeoTIFF in QGIS and overlay with the shapefiles for detailed inspection.\n",
    "- Adjust the probability threshold (currently 0.7) based on domain expertise.\n",
    "- Consider running the full national extent in batches if resources allow.\n",
    "- Use the uncertainty map to prioritize field survey locations.\n",
    "- Continue with Notebook 06 (Multi-Region Extension).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}